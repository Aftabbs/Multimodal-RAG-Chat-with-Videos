# Multimodal RAG with Videos

![image](https://github.com/user-attachments/assets/50a68ac6-5720-4dd9-bd29-ff6a12d16703)


## ğŸ”¹ Project Overview

This project demonstrates a **Multimodal Retrieval Augmented Generation (RAG)** pipeline designed to work directly with video content.
Instead of limiting retrieval to text, it integrates **both visual and linguistic components** â€” allowing you to search, retrieve, and generate answers from a rich pool of video data.

---

## ğŸ”¹ Why?

In a world where knowledge is increasingly represented in video format â€” webinars, lectures, meetings, vlogs â€” traditional text-centric search methods miss a large portion of information.
This pipeline lets you:

âœ… Handle **videos with subtitles or without subtitles**.
âœ… Combine **vision and audio modalities** alongside text.
âœ… Provide contextually rich answers by retrieving segments from both the videoâ€™s dialogue and its visual content.

---

## ğŸ”¹ Applications

* Educational platforms (searchable video lectures).
* Customer service (find segments in training recordings).
* Security or surveillance (query video footage for specific events).
* Content creators (quickly retrieve scenes or quotes).

---

## ğŸ”¹ Features

* **Support for multiple scenarios:**

  * **With transcript:** parses subtitles directly.
  * **With audio only:** performs Speech-to-Text first.
  * **With no language:** relies on Visual Embedding.

* **Multimodal Embedding:**

  * Text and Audio components from subtitles or Speech-to-Text.
  * Visual components from video segments.

* **Scalable vector storage:**

  * Stores embeddings in **LanceDB**, a lightweight, scalable vector database.

* **Framework:**

  * Built on **LangChain**, **Langchain-Community**, and **OpenAI-compatible LVLMs**.

---

## ğŸ”¹ Architectural Flow

![image](https://github.com/user-attachments/assets/f48d7e56-0505-4acf-9248-28d9996aa47f)

![image](https://github.com/user-attachments/assets/b60580e3-4a81-47a0-8e58-31decbbd0654)

![image](https://github.com/user-attachments/assets/abd031a8-cc52-4706-b007-b1639b5733fe)

```text
[Video Input]
 â””â”€ If subtitles present â†’ extract text directly
 â””â”€ If no subtitles â†’ extract audio, perform Speech-to-Text
 â””â”€ If no speech â†’ fallback to Visual Embedding (frames)


[Preprocessing]
 â””â”€ Split into segments (frames or subtitles)

[Embedding]
 â””â”€ Text segments â†’ Text Embedding Model
 â””â”€ Visual segments â†’ LVLM Visual Embedding Model
 â””â”€ Audio segments â†’ Speech Embedding Model

[Storage]
 â””â”€ All embeddings are indexed into **LanceDB** vector store

[Query]
 â””â”€ User's Query converts to embedding
 â””â”€ LanceDB performs similarity search across multimodal segments

[Response]
 â””â”€ Top-K segments retrieved
 â””â”€ LangChain constructs context for Large Language Model (LLM)

[Final]
 â””â”€ LLM generates a contextually rich, coherent answer
```

---

## ğŸ”¹ Tech Stack

âœ… **Frameworks:**

* \[LangChain, Langchain-Community] â€“ orchestration & retrieval
* \[LanceDB] â€“ vector storage
* \[OpenAI-compatible LVLM] â€“ multi-modal embedding
* \[moviepy, youtube-transcript-api, pydub] â€“ video and audio processing
* \[Python ecosystem] â€“ pandas, numpy, scikit-learn, etc.

âœ… **Other components:**

* \[WebVTT] for subtitles
* \[ffmpeg] under moviepy for audio and video processing
* \[OpenCV] for frame extraction when no subtitles or audio available

---

## ğŸ”¹ Project Flow Summary

|         | Process                                          |
| ------- | ------------------------------------------------ |
| **1ï¸âƒ£** | Extract subtitles or audio from video            |
| **2ï¸âƒ£** | Split into segments (frames, text, audio)        |
| **3ï¸âƒ£** | Convert segments into embeddings                 |
| **4ï¸âƒ£** | Store embeddings in vector database (LanceDB)    |
| **5ï¸âƒ£** | Query by transforming questions into embedding   |
| **6ï¸âƒ£** | Retrieve most relevant segments                  |
| **7ï¸âƒ£** | Combine context and generate answers with an LLM |

---

## ğŸ”¹ Possible Enhangements

âœ… Handle multi-language subtitles
âœ… Integrate automatic scene detection for more semantic segments
âœ… Combine audio, text, and video signals into unified embedding
âœ… Support Q\&A over large video collections
âœ… Provide UI with Gradio or Streamlit for interactive search

---

## ğŸ”¹ Summary

This pipeline lets you leverage all components of a video â€” its audio, subtitles, and visuals â€” to perform semantic search and Q\&A in a way pure text or audio processing canâ€™t match.
Using multimodal retrieval, you can uncover rich context from vast video archives quickly and accurately.





